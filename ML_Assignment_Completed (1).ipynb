{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJcXC50QJ9uA"
      },
      "source": [
        "# Foundations of Machine Learning and EDA — Completed Assignment (Colab Notebook)\n",
        "\n",
        "**Student:** _(KARAN KUMAR VERMA)_  \n",
        "**Assignment Code:** DA-AG-007  \n",
        "**Generated on:** 2025-11-02 10:38:22\n",
        "\n",
        "---\n"
      ],
      "id": "dJcXC50QJ9uA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dseD60p6J9uE"
      },
      "source": [
        "## Question 1 : What is the difference between AI, ML, DL, and Data Science? Provide a brief explanation of each.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Artificial Intelligence (AI)**: Broad field focused on creating machines or systems that can perform tasks that normally require human intelligence — reasoning, planning, perception, natural language understanding. Techniques: symbolic AI, rule-based systems, search, optimization, ML.\n",
        "\n",
        "**Machine Learning (ML)**: Subset of AI where systems improve performance on tasks by learning from data rather than being explicitly programmed. Techniques: supervised, unsupervised, reinforcement learning; common algorithms: linear/logistic regression, decision trees, SVM, ensemble methods.\n",
        "\n",
        "**Deep Learning (DL)**: Subset of ML that uses deep neural networks (many layers) to learn hierarchical representations from large datasets. Powerful for images, audio, text. Techniques: CNNs, RNNs/Transformers, autoencoders.\n",
        "\n",
        "**Data Science**: Interdisciplinary field combining domain knowledge, data engineering, statistics, visualization, and ML to extract insights and build data products. Involves data cleaning, EDA, modeling, deployment, and communication.\n",
        "\n",
        "**Scope / Techniques / Applications**\n",
        "- Scope: AI (broad) > ML (learning from data) > DL (neural-network-based models). Data Science overlaps with ML and AI but focuses on the full data lifecycle and business insight.\n",
        "- Techniques: AI includes logic/search/ML; ML includes statistical models and algorithms; DL focuses on neural architectures; Data Science uses statistics, visualization, ML, and engineering.\n",
        "- Applications: AI (autonomous systems, game AIs), ML (predictive analytics, recommendation systems), DL (image recognition, NLP), Data Science (business intelligence, experimental analysis).\n",
        "\n"
      ],
      "id": "dseD60p6J9uE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxLX4It4J9uH"
      },
      "source": [
        "## Question 2: Explain overfitting and underfitting in ML. How can you detect and prevent them?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Underfitting**: Model is too simple to capture underlying patterns (high bias, low variance). Training and validation errors are both high.\n",
        "**Overfitting**: Model learns noise/irrelevant patterns from training data (low bias, high variance). Training error is low but validation/test error is high.\n",
        "\n",
        "**Detection**:\n",
        "- Compare training vs validation loss/accuracy. Large gap (low train error, high val error) → overfitting. Both high → underfitting.\n",
        "- Learning curves (plot error vs training set size) help diagnose.\n",
        "\n",
        "**Prevention / Remedies**:\n",
        "- For underfitting: increase model complexity (more features, deeper model), reduce regularization, add feature interactions.\n",
        "- For overfitting: use regularization (L1/L2), reduce model complexity, use dropout (DL), data augmentation (images), increase training data, early stopping, cross-validation (k-fold) to get robust estimate.\n",
        "- Use ensemble methods (bagging reduces variance) and model selection via cross-validation.\n",
        "\n",
        "**Bias–Variance tradeoff**: Regularization moves model toward higher bias/lower variance; model selection aims to find a sweet spot minimizing expected generalization error.\n",
        "\n"
      ],
      "id": "RxLX4It4J9uH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWm-b35RJ9uI"
      },
      "source": [
        "## Question 3: How would you handle missing values in a dataset? Explain at least three methods with examples.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "1. **Deletion**: Remove rows (listwise deletion) or columns with missing values.\n",
        "   - Use when missingness is small and random (MCAR). Example: `df.dropna()`.\n",
        "\n",
        "2. **Simple Imputation**: Replace missing values with a statistic (mean/median/mode).\n",
        "   - Mean/median for numeric columns: `df['col'].fillna(df['col'].median())`.\n",
        "   - Mode for categorical variables.\n",
        "   - Works well when distribution is not strongly skewed (mean) or when outliers exist (median).\n",
        "\n",
        "3. **Predictive Imputation (Model-based)**: Train a model to predict missing values using other features (e.g., regression, k-NN imputation).\n",
        "   - Example: use `IterativeImputer` or `KNNImputer` from sklearn.\n",
        "\n",
        "4. **Indicator for Missingness**: Create a boolean flag column indicating missingness (useful if missingness itself is informative).\n",
        "\n",
        "5. **Advanced**: Multiple Imputation (such as MICE) to account for uncertainty in imputed values.\n",
        "\n",
        "Choose method depending on missingness mechanism (MCAR, MAR, MNAR) and fraction missing.\n",
        "\n"
      ],
      "id": "RWm-b35RJ9uI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJrRftiAJ9uJ"
      },
      "source": [
        "## Question 4: What is an imbalanced dataset? Describe two techniques to handle it (theoretical + practical).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Imbalanced dataset**: When classes are not represented equally (e.g., fraud detection where fraud examples are rare). Classifiers can be biased toward majority class.\n",
        "\n",
        "**Techniques**:\n",
        "1. **Resampling**\n",
        "   - **Oversampling**: Increase minority class samples (random oversampling, SMOTE — Synthetic Minority Oversampling Technique). Practical: `imblearn.over_sampling.SMOTE()`.\n",
        "   - **Undersampling**: Reduce majority class samples (random undersampling, Tomek links).\n",
        "\n",
        "2. **Algorithmic approaches**\n",
        "   - **Class weights / cost-sensitive learning**: Penalize mistakes on minority class more (e.g., `class_weight='balanced'` in sklearn models).\n",
        "   - **Ensemble methods**: Use balanced bagging or boosting tailored for imbalance (e.g., `BalancedRandomForest`, `AdaBoost` with sample weighting).\n",
        "\n",
        "**Evaluation metrics**: Use precision, recall, F1-score, ROC-AUC, PR-AUC instead of accuracy.\n",
        "\n"
      ],
      "id": "xJrRftiAJ9uJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkP-nIT6J9uJ"
      },
      "source": [
        "## Question 5: Why is feature scaling important in ML? Compare Min-Max scaling and Standardization.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Why scaling matters**:\n",
        "- Many algorithms rely on distance or gradient descent (KNN, K-means, SVM, logistic regression, neural networks). Features with larger scales can dominate the objective.\n",
        "- Scaling helps faster convergence in gradient-based optimizers.\n",
        "\n",
        "**Min-Max Scaling (Normalization)**\n",
        "- Transforms features to a fixed range, usually [0,1]: `X_scaled = (X - X.min)/(X.max - X.min)`.\n",
        "- Preserves shape of distribution but is sensitive to outliers.\n",
        "\n",
        "**Standardization (Z-score)**\n",
        "- Centers to zero mean and unit variance: `X_scaled = (X - mean)/std`.\n",
        "- Not bounded to [0,1]; less sensitive to outliers than min-max in many cases.\n",
        "\n",
        "**Which to use**:\n",
        "- Use Standardization for algorithms assuming Gaussian-like features (linear models, many ML algorithms).\n",
        "- Use Min-Max when features must be in a bounded interval (e.g., neural network inputs when activation expects limited range) or when comparing to known bounds.\n",
        "\n"
      ],
      "id": "gkP-nIT6J9uJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtgyZNn5J9uK"
      },
      "source": [
        "## Question 6: Compare Label Encoding and One-Hot Encoding. When would you prefer one over the other?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Label Encoding**: Assigns integer labels to categories (e.g., Red→0, Green→1, Blue→2). Useful for ordinal categories where order matters (e.g., Low, Medium, High). But can introduce artificial ordinal relationship if used on nominal categories.\n",
        "\n",
        "**One-Hot Encoding**: Creates binary columns — one per category (e.g., is_red, is_green, is_blue). No ordinal assumptions; ideal for nominal categories.\n",
        "\n",
        "**When to prefer**:\n",
        "- Use Label Encoding for ordinal categorical features.\n",
        "- Use One-Hot Encoding for nominal features with limited cardinality.\n",
        "- For high-cardinality categorical variables, consider target encoding, embedding layers (in DL), or hashing trick.\n",
        "\n"
      ],
      "id": "YtgyZNn5J9uK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1qYQ5tNJ9uK"
      },
      "source": [
        "## Practical / Coding Questions (Q7–Q10)\n",
        "\n",
        "The following code cells are written to run in **Google Colab** (they will clone the dataset repository and perform EDA/visualizations). Run the notebook in Colab to see outputs. Each question is shown as a Markdown cell followed by code that performs the analysis."
      ],
      "id": "B1qYQ5tNJ9uK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjTufINjJ9uK"
      },
      "source": [
        "## Question 7: Google Play Store Dataset\n",
        "**Task:** a). Analyze the relationship between app categories and ratings. Which categories have the highest/lowest average ratings, and what could be the possible reasons?\n",
        "\n",
        "Dataset: https://github.com/MasteriNeuron/datasets.git\n",
        "\n",
        "**Answer (Code & EDA):**\n",
        "\n"
      ],
      "id": "EjTufINjJ9uK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19RZU9RDJ9uK"
      },
      "outputs": [],
      "source": [
        "# Question 7: Google Play Store Dataset analysis (run in Colab)\n",
        "# 1) Clone the dataset repo (Colab has internet)\n",
        "!git clone https://github.com/MasteriNeuron/datasets.git\n",
        "# 2) Load the dataset (update filename if different in repo)\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Try a few common filenames that such repos use. Adjust if needed.\n",
        "possible_files = [\n",
        "    'datasets/googleplaystore.csv',\n",
        "    'datasets/google_play_store.csv',\n",
        "    'datasets/googleplaystore_cleaned.csv',\n",
        "    'datasets/googleplaystore_full.csv'\n",
        "]\n",
        "\n",
        "for f in possible_files:\n",
        "    try:\n",
        "        df = pd.read_csv(f)\n",
        "        print('Loaded file:', f)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        df = None\n",
        "\n",
        "if df is None:\n",
        "    raise FileNotFoundError('Google Play Store dataset file not found in repo. Check filenames.')\n",
        "\n",
        "# Basic cleaning: ensure 'Category' and 'Rating' exist\n",
        "print(df.columns)\n",
        "if 'Category' not in df.columns or 'Rating' not in df.columns:\n",
        "    display(df.head())\n",
        "    raise KeyError('Expected columns Category and Rating not found — adapt to actual column names.')\n",
        "\n",
        "# Convert ratings to numeric and drop NaNs\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "cat_rating = df.groupby('Category')['Rating'].agg(['mean','median','count','std']).reset_index().sort_values('mean', ascending=False)\n",
        "print(cat_rating.head(10))\n",
        "print('\\nLowest average rating categories:')\n",
        "print(cat_rating.tail(10))\n",
        "\n",
        "# Plot average rating per category (only categories with at least N apps to avoid noise)\n",
        "min_apps = 20\n",
        "filtered = cat_rating[cat_rating['count'] >= min_apps].sort_values('mean')\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(filtered['Category'], filtered['mean'])\n",
        "plt.xlabel('Average Rating')\n",
        "plt.title('Average App Rating by Category (categories with >= {} apps)'.format(min_apps))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Possible reasons to include in writeup (to be expanded): category complexity, review bias, app maturity, user expectations, monetization.\n"
      ],
      "id": "19RZU9RDJ9uK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_PVWU6LJ9uL"
      },
      "source": [
        "## Question 8: Titanic Dataset\n",
        "**Tasks:**\n",
        "(a) Compare the survival rates based on passenger class (Pclass). Which class had the highest survival rate, and why?\n",
        "(b) Analyze how age (Age) affected survival. Group passengers into children (Age < 18) and adults (Age ≥ 18). Did children have a better chance of survival?\n",
        "\n",
        "Dataset: https://github.com/MasteriNeuron/datasets.git\n",
        "\n",
        "**Answer (Code & EDA):**\n",
        "\n"
      ],
      "id": "U_PVWU6LJ9uL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiV5_C4NJ9uM"
      },
      "outputs": [],
      "source": [
        "# Question 8: Titanic analysis (run in Colab)\n",
        "# Clone repo (if not already)\n",
        "# !git clone https://github.com/MasteriNeuron/datasets.git\n",
        "\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Try to locate titanic file\n",
        "possible = [\n",
        "    'datasets/titanic.csv',\n",
        "    'datasets/Titanic.csv',\n",
        "    'datasets/titanic_train.csv',\n",
        "    'datasets/titanic_train_cleaned.csv'\n",
        "]\n",
        "for f in possible:\n",
        "    try:\n",
        "        tit = pd.read_csv(f)\n",
        "        print('Loaded:', f)\n",
        "        break\n",
        "    except:\n",
        "        tit = None\n",
        "\n",
        "if tit is None:\n",
        "    raise FileNotFoundError('Titanic dataset not found in repo.')\n",
        "\n",
        "# Inspect columns\n",
        "print(tit.columns)\n",
        "# Expected 'Pclass', 'Survived', 'Age'\n",
        "if not {'Pclass','Survived','Age'}.issubset(set(tit.columns)):\n",
        "    display(tit.head())\n",
        "    raise KeyError('Expected Pclass, Survived, Age columns are missing; adjust column names.')\n",
        "\n",
        "# Survival rates by class\n",
        "pclass_surv = tit.groupby('Pclass')['Survived'].mean().reset_index().sort_values('Survived', ascending=False)\n",
        "print('Survival rate by Pclass:')\n",
        "print(pclass_surv)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(pclass_surv['Pclass'].astype(str), pclass_surv['Survived'])\n",
        "plt.xlabel('Pclass')\n",
        "plt.ylabel('Survival Rate')\n",
        "plt.title('Titanic: Survival Rate by Passenger Class')\n",
        "plt.show()\n",
        "\n",
        "# Age groups: children (<18) vs adults (>=18)\n",
        "tit['age_group'] = tit['Age'].apply(lambda x: 'child' if pd.notna(x) and x < 18 else ('adult' if pd.notna(x) else 'unknown'))\n",
        "age_surv = tit[tit['age_group']!='unknown'].groupby('age_group')['Survived'].mean().reset_index()\n",
        "print('\\nSurvival by age group:')\n",
        "print(age_surv)\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.bar(age_surv['age_group'], age_surv['Survived'])\n",
        "plt.title('Survival: Children vs Adults')\n",
        "plt.ylabel('Survival Rate')\n",
        "plt.show()\n",
        "\n",
        "# Short conclusions: write in markdown below the results.\n"
      ],
      "id": "EiV5_C4NJ9uM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy2uoFPoJ9uM"
      },
      "source": [
        "## Question 9: Flight Price Prediction Dataset\n",
        "**Tasks:**\n",
        "(a) How do flight prices vary with the days left until departure? Identify any exponential price surges and recommend the best booking window.\n",
        "(b) Compare prices across airlines for the same route (e.g., Delhi-Mumbai). Which airlines are consistently cheaper/premium, and why?\n",
        "\n",
        "Dataset: https://github.com/MasteriNeuron/datasets.git\n",
        "\n",
        "**Answer (Code & EDA):**\n",
        "\n"
      ],
      "id": "vy2uoFPoJ9uM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7ykF4n2J9uN"
      },
      "outputs": [],
      "source": [
        "# Question 9: Flight Price Prediction analysis (run in Colab)\n",
        "# !git clone https://github.com/MasteriNeuron/datasets.git\n",
        "\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Try to find flight dataset\n",
        "possible = [\n",
        "    'datasets/flight_price.csv',\n",
        "    'datasets/FlightPrice.csv',\n",
        "    'datasets/flights.csv',\n",
        "    'datasets/flight_price_dataset.csv'\n",
        "]\n",
        "for f in possible:\n",
        "    try:\n",
        "        flights = pd.read_csv(f)\n",
        "        print('Loaded:', f)\n",
        "        break\n",
        "    except:\n",
        "        flights = None\n",
        "\n",
        "if flights is None:\n",
        "    raise FileNotFoundError('Flight dataset not found in repo; check exact filename.')\n",
        "\n",
        "print(flights.columns)\n",
        "# Expected to have: 'Price', 'Days_left' or 'days_left' or 'days_until_flight', 'Airline', 'Route'\n",
        "# Normalize column names for safety\n",
        "cols = [c.lower() for c in flights.columns]\n",
        "print(cols)\n",
        "\n",
        "# Attempt to find days left column\n",
        "days_cols = [c for c in flights.columns if 'day' in c.lower()]\n",
        "price_cols = [c for c in flights.columns if 'price' in c.lower()]\n",
        "airline_cols = [c for c in flights.columns if 'airline' in c.lower()]\n",
        "route_cols = [c for c in flights.columns if 'route' in c.lower() or ('from' in c.lower() and 'to' in c.lower())]\n",
        "\n",
        "print('Detected cols candidates:\\nDays:', days_cols, '\\nPrice:', price_cols, '\\nAirline:', airline_cols, '\\nRoute:', route_cols)\n",
        "\n",
        "# Example analysis once correct columns located:\n",
        "# flights['days_left'] = flights[days_cols[0]]\n",
        "# flights['price'] = flights[price_cols[0]]\n",
        "# Group by days_left and compute median price\n",
        "# grp = flights.groupby('days_left')['price'].median().reset_index()\n",
        "# plt.figure(figsize=(8,5))\n",
        "# plt.plot(grp['days_left'], grp['price'])\n",
        "# plt.gca().invert_xaxis()  # so that 0 (day of flight) is rightmost if desired\n",
        "# plt.xlabel('Days left until departure')\n",
        "# plt.ylabel('Median Price')\n",
        "# plt.title('Price vs Days Left — look for exponential surges near departure')\n",
        "# plt.show()\n",
        "#\n",
        "# For route comparison, filter route e.g. 'Delhi-Mumbai' and compare airlines using boxplots or medians.\n"
      ],
      "id": "B7ykF4n2J9uN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOrrz3VJJ9uN"
      },
      "source": [
        "## Question 10: HR Analytics Dataset\n",
        "**Tasks:**\n",
        "(a) What factors most strongly correlate with employee attrition? Use visualizations to show key drivers (e.g., satisfaction, overtime, salary).\n",
        "(b) Are employees with more projects more likely to leave?\n",
        "\n",
        "Dataset: hr_analytics (link provided in assignment)\n",
        "\n",
        "**Answer (Code & EDA):**\n",
        "\n"
      ],
      "id": "IOrrz3VJJ9uN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mwfv9w-J9uO"
      },
      "outputs": [],
      "source": [
        "# Question 10: HR Analytics (run in Colab)\n",
        "# !git clone https://github.com/MasteriNeuron/datasets.git\n",
        "\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Try the known path from assignment\n",
        "try:\n",
        "    hr = pd.read_csv('datasets/hr_analytics.csv')\n",
        "    print('Loaded datasets/hr_analytics.csv')\n",
        "except Exception as e:\n",
        "    # try alternate filenames\n",
        "    found = False\n",
        "    for f in ['datasets/hr_analytics.csv','datasets/hr_analytics_final.csv','datasets/hr_analytics_clean.csv']:\n",
        "        try:\n",
        "            hr = pd.read_csv(f)\n",
        "            print('Loaded', f)\n",
        "            found = True\n",
        "            break\n",
        "        except:\n",
        "            pass\n",
        "    if not found:\n",
        "        raise FileNotFoundError('hr_analytics.csv not found in repo; check filenames.')\n",
        "\n",
        "print(hr.columns)\n",
        "# Expected columns might include: 'Attrition' (or 'left'), 'satisfaction', 'satisfaction_level', 'last_evaluation', 'average_montly_hours', 'time_spend_company', 'number_project', 'salary', 'overtime' etc.\n",
        "# Basic correlation with Attrition (convert to numeric if needed)\n",
        "if 'Attrition' in hr.columns:\n",
        "    hr['attrition_flag'] = hr['Attrition'].map(lambda x: 1 if str(x).strip().lower() in ['yes','true','1'] else 0)\n",
        "elif 'left' in hr.columns:\n",
        "    hr['attrition_flag'] = hr['left'].astype(int)\n",
        "else:\n",
        "    # try to infer\n",
        "    display(hr.head())\n",
        "    raise KeyError('Could not find Attrition/left column; adjust to actual dataset.')\n",
        "\n",
        "# Correlation matrix for numeric features vs attrition\n",
        "num = hr.select_dtypes(include=[np.number])\n",
        "corr_with_attr = num.corr()['attrition_flag'].abs().sort_values(ascending=False)\n",
        "print('Top correlations with attrition:')\n",
        "print(corr_with_attr.head(10))\n",
        "\n",
        "# Visualize key drivers (example: satisfaction, overtime, salary)\n",
        "# Example plot for number_project vs attrition rate\n",
        "if 'number_project' in hr.columns:\n",
        "    proj_grp = hr.groupby('number_project')['attrition_flag'].mean().reset_index()\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(proj_grp['number_project'], proj_grp['attrition_flag'], marker='o')\n",
        "    plt.xlabel('Number of Projects')\n",
        "    plt.ylabel('Attrition Rate')\n",
        "    plt.title('Attrition rate vs number of projects')\n",
        "    plt.show()\n",
        "\n",
        "# Salary vs attrition (if salary exists and is categorical like low, medium, high)\n",
        "if 'salary' in hr.columns:\n",
        "    display(hr.groupby('salary')['attrition_flag'].mean())\n",
        "\n",
        "# Overtime or 'overtime' column effect\n",
        "if 'overtime' in hr.columns:\n",
        "    display(hr.groupby('overtime')['attrition_flag'].mean())\n"
      ],
      "id": "9mwfv9w-J9uO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N85R1scjJ9uO"
      },
      "source": [
        "----\n",
        "\n",
        "### Submission\n",
        "- This notebook contains every question followed directly by its answer (theory or a code cell) as requested.\n",
        "- To run the analyses, open this notebook in **Google Colab**, run the cells (Colab will clone the repository and execute the analysis code), and then **File → Download → Download .ipynb** or **Save a copy in Drive**.\n",
        "\n",
        "Good luck — if you want, I can also (a) run the analyses here and paste results (but note this environment has no internet), or (b) convert this notebook to PDF after you run it in Colab and share the PDF.\n"
      ],
      "id": "N85R1scjJ9uO"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}